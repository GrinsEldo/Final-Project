# Phase 2: Data Loading and Splitting
import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
column_names = [
    "age", "workclass", "fnlwgt", "education", "education-num", "marital-status", 
    "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", 
    "hours-per-week", "native-country", "income"
]

# Read the dataset into a pandas DataFrame
data = pd.read_csv(url, names=column_names, sep=",\s*", engine="python")

# Display the first few rows of the dataset
print(data.head())

# Preprocessing: Encoding the target variable
data['income'] = data['income'].apply(lambda x: 1 if x == ">50K" else 0)

# Handle missing values (if any)
data = data.dropna()

# Splitting the data into features (X) and target variable (y)
X = data.drop('income', axis=1)
y = data['income']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shapes of the split data
print(f"Training data size: {X_train.shape}, Test data size: {X_test.shape}")

# Phase 3: Model Building and Hyperparameter Tuning
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Initialize the Random Forest classifier
rf_model = RandomForestClassifier(random_state=42)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True, False]
}

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters and model after tuning
print("Best hyperparameters:", grid_search.best_params_)

# Retrain the best model on the training data
best_rf_model = grid_search.best_estimator_

# Evaluate the model
y_pred = best_rf_model.predict(X_test)

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))


# Phase 4: Finalizing the Model and Making Predictions
import joblib

# Save the model to a file
joblib.dump(best_rf_model, 'random_forest_model.pkl')

# Make predictions on the test set
predictions = best_rf_model.predict(X_test)

# Evaluate the model on the test data
from sklearn.metrics import accuracy_score, confusion_matrix

# Accuracy of the model
accuracy = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Load the saved model (if needed later)
loaded_model = joblib.load('random_forest_model.pkl')

# Example prediction for new data (if available)
# new_data = [[...]]  # New feature data goes here
# prediction = loaded_model.predict(new_data)
# print(f"Prediction for new data: {prediction}")
